{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhsOr8Kabz9A",
        "outputId": "e0a93403-6903-46cf-95fd-1bfde133a8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.0.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (18.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.32.3)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (3.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.14.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2025.4.26)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "CTmawPSQcDXQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "On running this cell , we well get the\n",
        "  - Episode Metadata\n",
        "  - Joint Positions\n",
        "  - Cartesian Positions\n",
        "  - Left Images\n",
        "  - Right Images\n",
        "\"\"\"\n",
        "\n",
        "ds = tfds.load(\"droid_100\", data_dir=\"gs://gresearch/robotics\", split=\"train\")\n",
        "\n",
        "output_file = \"joint_ps.txt\"\n",
        "cartesian_file = \"cart_pos.txt\"\n",
        "image_folder = \"images_right\"\n",
        "image_left_folder = \"images_left\"\n",
        "zip_file_1 = \"images_left.zip\"\n",
        "zip_file_2 = \"images_right.zip\"\n",
        "\n",
        "\"\"\"\n",
        "I think there are 100 scenes in the RLDS => so any scene_no below that should work => But check that out once pls ..\n",
        "\"\"\"\n",
        "\n",
        "scene_no = int(input(\"Enter the scene number to process: \"))\n",
        "\n",
        "os.makedirs(image_folder, exist_ok=True)\n",
        "os.makedirs(image_left_folder, exist_ok=True)\n",
        "\n",
        "image_count_1 = 0\n",
        "image_count_2 = 0\n",
        "\n",
        "with open(output_file, \"w\") as joint_file, open(cartesian_file, \"w\") as cartesian_file:\n",
        "    for episode_index, episode in enumerate(ds):\n",
        "        if episode_index == scene_no:\n",
        "            metadata = episode[\"episode_metadata\"]\n",
        "            joint_file.write(\"Episode Metadata:\\n\")\n",
        "            joint_file.write(f\"{metadata}\\n\\n\")\n",
        "            print(\"Episode Metadata:\")\n",
        "            print(metadata)\n",
        "\n",
        "            joint_file.write(\"Joint Positions:\\n\")\n",
        "            cartesian_file.write(\"Cartesian Positions:\\n\")\n",
        "            for step_index, step in enumerate(episode[\"steps\"]):\n",
        "                # Joint positions\n",
        "                joint_position = step[\"observation\"][\"joint_position\"].numpy()\n",
        "                joint_file.write(f\"{joint_position.tolist()}\\n\")\n",
        "\n",
        "                # Cartesian position\n",
        "                cartesian_position = step[\"observation\"][\"cartesian_position\"].numpy()\n",
        "                cartesian_file.write(f\"{cartesian_position.tolist()}\\n\")\n",
        "\n",
        "                # Left images ..\n",
        "                image_array_1 = step[\"observation\"][\"exterior_image_1_left\"].numpy()\n",
        "                image_1 = Image.fromarray(np.uint8(image_array_1))\n",
        "                image_path_1 = os.path.join(image_folder, f\"image_{step_index:05d}.png\")\n",
        "                image_1.save(image_path_1)\n",
        "                image_count_1 += 1\n",
        "\n",
        "                # Right images ..\n",
        "                image_array_2 = step[\"observation\"][\"exterior_image_2_left\"].numpy()\n",
        "                image_2 = Image.fromarray(np.uint8(image_array_2))\n",
        "                image_path_2 = os.path.join(image_left_folder, f\"image_{step_index:05d}.png\")\n",
        "                image_2.save(image_path_2)\n",
        "                image_count_2 += 1\n",
        "            break\n",
        "\n",
        "with zipfile.ZipFile(zip_file_1, \"w\", zipfile.ZIP_DEFLATED) as zipf_1:\n",
        "    for root, _, files in os.walk(image_folder):\n",
        "        for file in files:\n",
        "            zipf_1.write(os.path.join(root, file),\n",
        "                         os.path.relpath(os.path.join(root, file), image_folder))\n",
        "\n",
        "with zipfile.ZipFile(zip_file_2, \"w\", zipfile.ZIP_DEFLATED) as zipf_2:\n",
        "    for root, _, files in os.walk(image_left_folder):\n",
        "        for file in files:\n",
        "            zipf_2.write(os.path.join(root, file),\n",
        "                         os.path.relpath(os.path.join(root, file), image_left_folder))\n",
        "\n",
        "print(f\"Episode metadata and joint positions have been saved in '{output_file}'.\")\n",
        "print(f\"Cartesian positions have been saved in '{cartesian_file}'.\")\n",
        "print(f\"{image_count_1} images from 'exterior_image_1_left' saved in '{zip_file_1}'.\")\n",
        "print(f\"{image_count_2} images from 'exterior_image_2_left' saved in '{zip_file_2}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkajhogHb21E",
        "outputId": "16d2ee37-6123-4953-e489-568c368300e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the scene number to process: 4\n",
            "Episode Metadata:\n",
            "{'file_path': <tf.Tensor: shape=(), dtype=string, numpy=b'/nfs/kun2/datasets/r2d2/r2d2-data-full/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/trajectory.h5'>, 'recording_folderpath': <tf.Tensor: shape=(), dtype=string, numpy=b'/nfs/kun2/datasets/r2d2/r2d2-data-full/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/MP4'>}\n",
            "Episode metadata and joint positions have been saved in 'joint_ps.txt'.\n",
            "Cartesian positions have been saved in '<_io.TextIOWrapper name='cart_pos.txt' mode='w' encoding='utf-8'>'.\n",
            "213 images from 'exterior_image_1_left' saved in 'images_left.zip'.\n",
            "213 images from 'exterior_image_2_left' saved in 'images_right.zip'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The RLDS data doesn't contain any of the intrinsic info readily , so we need to search for that in the RAW data using the episode metadata path ..\n",
        "\n",
        "From the above cell output :  numpy=b'/nfs/kun2/datasets/r2d2/r2d2-data-full/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/MP4'\n",
        "\n",
        "Now , search for this path in the RAW data ..\n",
        "\"\"\"\n",
        "\n",
        "!gsutil ls gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P43ycsjxcOvp",
        "outputId": "e589751c-f43f-4154-8949-1039b4f57c6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/14549178.svo\n",
            "gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/20252535.svo\n",
            "gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/25947356.svo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TO get the extrinsics , we need to download the trajectory.h5 file and then check the content inside it\n",
        "\"\"\"\n",
        "\n",
        "!gsutil -m cp -r gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/trajectory.h5 ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsWgic59hdib",
        "outputId": "a740dc6c-e7f7-4e99-866b-106660d7c0c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/trajectory.h5...\n",
            "/ [0/1 files][    0.0 B/  1.2 MiB]   0% Done                                    \r/ [1/1 files][  1.2 MiB/  1.2 MiB] 100% Done                                    \r\n",
            "Operation completed over 1 objects/1.2 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This .svo will contain the intrinsic info (K matrix which we will subsitute before running the motion code)\n",
        "!gsutil -m cp -r gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/20252535.svo ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVEBbS19d2Iw",
        "outputId": "0247af7a-8264-4b47-f7fc-2ab3ed3bc48e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://gresearch/robotics/droid_raw/1.0.1/TRI/success/2023-09-21/Thu_Sep_21_17:26:41_2023/recordings/SVO/20252535.svo...\n",
            "/ [1/1 files][  9.3 MiB/  9.3 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/9.3 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "nHVyEPTjeGv6"
      }
    }
  ]
}